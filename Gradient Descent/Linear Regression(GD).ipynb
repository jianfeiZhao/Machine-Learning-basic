{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37132bitbasecondac57ed9f10db04ac8b75f9259c7f2062d",
   "display_name": "Python 3.7.1 32-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "def grad_descent(w, X, y, lr, itr):\n",
    "# input:\n",
    "#       w - initial w\n",
    "#       X - Data vector X\n",
    "#       y - Target value (label)\n",
    "#       lr - Learning rate\n",
    "#       itr - No. of iterations\n",
    "# output:\n",
    "#       w_star - final solution\n",
    "  for _ in range(itr):\n",
    "# Tip:  \n",
    "#       We can quickly compute matrix multiplication in python using '@' - operator. Transpose          can be quickly computed with '.T' - operator\n",
    "    y_hat = X@w\n",
    "    grad = (X.T)@(y_hat-y) # compute gradient\n",
    "    w = w - lr*grad        # update\n",
    "\n",
    "# Break condition: \n",
    "#       we check if the gradient is smaller than some threshold, if true, we break the loop and         return the solution, otherwise we keep iterating\n",
    "    if np.linalg.norm(grad) < 1e-4:\n",
    "      break\n",
    "\n",
    "# print grad norm after some iterations\n",
    "    if _ % 1000 ==0:\n",
    "      print(\"itr: {:5d}, ||grad||: {:0.6E}\".format(_, np.linalg.norm(grad)))\n",
    "  return w\n",
    "\n",
    "# Load diabetes dataset\n",
    "data = datasets.load_diabetes()\n",
    "X, Y = data.data, data.target.reshape(-1,1)\n",
    "\n",
    "# splitting dataset in (80%) training and (20%)testing\n",
    "total_size = X.shape[0]\n",
    "split_idx = int(0.80*total_size)\n",
    "X_train, Y_train = X[0:split_idx,:], Y[0:split_idx]\n",
    "X_test, Y_test = X[split_idx:,:], Y[split_idx:]\n",
    "\n",
    "# Adding a column of ones\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0],1)), X_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0],1)), X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "itr:     0, ||grad||: 5.314557E+04\nitr:  1000, ||grad||: 5.885787E+01\nitr:  2000, ||grad||: 1.073597E+01\nitr:  3000, ||grad||: 5.016284E+00\nitr:  4000, ||grad||: 4.044244E+00\nitr:  5000, ||grad||: 3.751670E+00\nitr:  6000, ||grad||: 3.584457E+00\nitr:  7000, ||grad||: 3.458024E+00\nitr:  8000, ||grad||: 3.353992E+00\nitr:  9000, ||grad||: 3.264982E+00\n"
    }
   ],
   "source": [
    "# define random w\n",
    "w0 = np.random.normal(size=(11,1))\n",
    "lr = 0.003\n",
    "itr = 10000\n",
    "\n",
    "# compute w_star using gradient descent\n",
    "w_star = grad_descent(w0, X_train, Y_train, lr, itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.535278857277548\n"
    }
   ],
   "source": [
    "#compute RSS on test data\n",
    "y_pred = X_test@w_star\n",
    "RSS = np.mean((y_pred-Y_test)**2)/(np.std(Y_test)**2)\n",
    "Rsq = 1 - RSS\n",
    "print(Rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
